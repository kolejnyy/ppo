{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "import numpy as np\n",
    "from numpy import random, sqrt\n",
    "\n",
    "# Network libraries\n",
    "import torch\n",
    "import torch.nn.functional as F, torch.nn as nn, torch.optim as optim\n",
    "from torch.distributions import Categorical as Categorical\n",
    "\n",
    "# Game library\n",
    "from TicTacToe import initialState, randomState, printState, drawState, move, possibleMoves, possibleMovesMask, isOver, gameScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "\t# Initialize the actor and critic\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(ActorCritic, self).__init__()\n",
    "\n",
    "\t\t# Actor\n",
    "\t\tself.actor = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 8, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(8, 32, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(32, 9)\n",
    "\t\t)\n",
    "\n",
    "\t\t# Critic\n",
    "\t\tself.critic = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 8, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(8, 32, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(32, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\t# Instead of using a forward function, we will split it into \n",
    "\t# one action for the actor, and one for the critic\n",
    "\tdef forward(self):\n",
    "\t\traise NotImplementedError\n",
    "\t\n",
    "\n",
    "\t# Obtain the probabilities of actions and a chosen action\n",
    "\tdef act(self, state):\n",
    "\t\t# Change the state into suitable format\n",
    "\t\ttensor_state = torch.from_numpy(state.reshape(-1, 1, 3,3)).float()\n",
    "\n",
    "\t\t# Calculate the initial output of the actor\n",
    "\t\tprobs = self.actor(tensor_state)\n",
    "\t\t\n",
    "\t\t# Apply the invalid action masking\n",
    "\t\tprobs = torch.where(possibleMovesMask(state, 1), probs, torch.tensor([-1e8]*9))\n",
    "\n",
    "\t\t# Apply the softmax function to obtain probabilities\n",
    "\t\tprobs = nn.Softmax(dim=-1)(probs)\n",
    "\n",
    "\t\t# Create a distribution\n",
    "\t\tdist = Categorical(probs)\n",
    "\n",
    "\t\t# Pick an action using the generated distribution\n",
    "\t\taction = dist.sample()\n",
    "\n",
    "\t\t# Evaluate the log_prob of the chosen action\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\n",
    "\t\t# Return the action and the log_probabilties\n",
    "\t\treturn action.detach(), log_prob.detach()\n",
    "\n",
    "\n",
    "\t# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4]), tensor([-2.1596]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNet = ActorCritic()\n",
    "\n",
    "state = randomState()\n",
    "\n",
    "myNet.act(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a283dd9a253759e7df9c9142bbdf863a8a5c2da97ed0cffee6dbf6a32555f09f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
