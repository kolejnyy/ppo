{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "import numpy as np\n",
    "from numpy import random, sqrt\n",
    "\n",
    "# Network libraries\n",
    "import torch\n",
    "import torch.nn.functional as F, torch.nn as nn, torch.optim as optim\n",
    "from torch.distributions import Categorical as Categorical\n",
    "\n",
    "# Game library\n",
    "from TicTacToe import initialState, randomState, printState, drawState, move, possibleMoves, possibleMovesMask, isOver, gameScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Advantage Estimate (GAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAE is calculated using the formula:\n",
    "\n",
    "$ \\begin{equation*}\n",
    "\\hat{A_t}^{GAE(\\gamma, \\lambda)} = (1-\\lambda)\\left( \\hat{A}^{(1)}_t + \\lambda\\hat{A}^{(2)}_t + \\lambda^2\\hat{A}^{(3)}_t + ...\\right) = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta^V_{t+l}\n",
    "\\end{equation*}$\n",
    "\n",
    "where $\\delta^V_{t}$ is the TD residual defined by:\n",
    "\n",
    "$ \\begin{equation*}\n",
    "\\delta^V_{t} = r_t + \\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "Note that by the definition, we can easily evaulate $ \\hat{A_t}^{GAE(\\gamma, \\lambda)} $ based on $ \\hat{A_{t+1}}^{GAE(\\gamma, \\lambda)} $:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{A_t}^{GAE(\\gamma, \\lambda)} = \\delta^V_t + \\gamma\\lambda \\hat{A}_{t+1}^{GAE(\\gamma, \\lambda)} \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae(values, rewards, lmbda = 0.95, gamma = 0.99):\n",
    "\t# Initialize the advangate\n",
    "\tadv = 0\n",
    "\t# Initialize the results\n",
    "\tresults = []\n",
    "\n",
    "\t# Starting with the last action, iterate over actions and calculate the advantages\n",
    "\tfor t in reversed(range(len(rewards))):\n",
    "\t\t# Calculate the current value of delta\n",
    "\t\tdelta = rewards[t] + gamma * values[t+1] - values[t]\n",
    "\t\t# Update the current advantage\n",
    "\t\tadv = delta + gamma * lmbda * adv\n",
    "\t\t# Add the estimates advantage to the value, to obtain a predicition of the real value\n",
    "\t\tresults.append(adv+values[t])\n",
    "\t\n",
    "\t# Reverse the results to obtain the right order\n",
    "\tresults.reverse()\n",
    "\n",
    "\t# Return the results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "\t# Initialize the actor and critic\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(ActorCritic, self).__init__()\n",
    "\n",
    "\t\t# Actor\n",
    "\t\tself.actor = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 8, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(8, 32, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(32, 9)\n",
    "\t\t)\n",
    "\n",
    "\t\t# Critic\n",
    "\t\tself.critic = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 8, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(8, 32, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(32, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\t\n",
    "\t# Instead of using a forward function, we will split it into \n",
    "\t# one action for the actor, and one for the critic\n",
    "\tdef forward(self):\n",
    "\t\traise NotImplementedError\n",
    "\t\n",
    "\n",
    "\t# For a given state, return an action according to a current policy and the log_prob\n",
    "\t# of performing this action\n",
    "\tdef act(self, state):\n",
    "\t\t# Change the state into suitable format\n",
    "\t\ttensor_state = torch.reshape(state, (-1,1,3,3))#torch.from_numpy(state.reshape(-1, 1, 3,3)).float()\n",
    "\n",
    "\t\t# Calculate the initial output of the actor\n",
    "\t\tprobs = self.actor(tensor_state)\n",
    "\t\t\n",
    "\t\t# Apply the invalid action masking\n",
    "\t\tprobs = torch.where(possibleMovesMask(state, 1), probs, torch.tensor([-1e8]*9))\n",
    "\n",
    "\t\t# Apply the softmax function to obtain probabilities\n",
    "\t\tprobs = self.softmax(probs)\n",
    "\n",
    "\t\t# Create a distribution\n",
    "\t\tdist = Categorical(probs)\n",
    "\n",
    "\t\t# Pick an action using the generated distribution\n",
    "\t\taction = dist.sample()\n",
    "\n",
    "\t\t# Evaluate the log_prob of the chosen action\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\n",
    "\t\t# Return the action and the log_probabilties\n",
    "\t\treturn action.detach(), log_prob.detach()\n",
    "\n",
    "\n",
    "\t# For a given state and action, return:\n",
    "\t# \t- \tthe log_probabilities of the action, according to the current policy, \n",
    "\t# \t-\tthe estimated value of the state, according to the critic,\n",
    "\t#\t-\tthe entropy of the distribution\n",
    "\tdef evaluate(self, state, action):\n",
    "\t\t# Change the state into suitable format\n",
    "\t\ttensor_state = torch.reshape(state, (-1,1,3,3))#torch.from_numpy(state.reshape(-1, 1, 3,3)).float()\n",
    "\n",
    "\t\t# Calculate the initial output of the actor\n",
    "\t\tprobs = self.actor(tensor_state)\n",
    "\t\t\n",
    "\t\t# Apply the invalid action masking\n",
    "\t\tprobs = torch.where(possibleMovesMask(state, 1), probs, torch.tensor([-1e8]*9))\n",
    "\n",
    "\t\t# Apply the softmax function to obtain probabilities\n",
    "\t\tprobs = self.softmax(probs)\n",
    "\n",
    "\t\t# Create a distribution\n",
    "\t\tdist = Categorical(probs)\n",
    "\n",
    "\t\t# Evaluate the log_prob of the chosen action\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\n",
    "\t\t# Calculate the entropy of the distribution\n",
    "\t\tdist_entropy = dist.entropy()\n",
    "\n",
    "\t\t# Calculate the evaluation of the position\n",
    "\t\tstate_eval = self.critic(tensor_state)\n",
    "\n",
    "\t\t# Return the action and the log_probabilties\n",
    "\t\treturn log_prob, state_eval, dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Objectiive function parameters\n",
    "c_1 = 0.5\n",
    "c_2 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network model\n",
    "model = ActorCritic()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# MSE Loss\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For given states, actions, log_probabilities of actions, advantages and predicted values,\n",
    "# prepare batches of a given size\n",
    "def generate_batches(states, actions, log_probs, advantages, pred_values, batch_size):\n",
    "\t# Evaluate the number of batches that we can create\n",
    "\tnum_batches = (states.size(0))//batch_size\n",
    "\t# Create a permutation of indices\n",
    "\tperm = np.array(range(len(states)))\n",
    "\tnp.random.shuffle(perm)\n",
    "\n",
    "\t# Based on the permutation, prepare and yield batches\n",
    "\tfor i in range(num_batches):\n",
    "\t\t_range = perm[i*batch_size:(i+1)*batch_size]\n",
    "\t\tyield states[_range], actions[_range], log_probs[_range], advantages[_range], pred_values[_range]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO aims to maximize the following objective function:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_t^{CLIP+VF+S}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ L_t^{CLIP}(\\theta) - c_1 L_t^{VF}(\\theta) + c_2 S[\\pi_\\theta](s_t) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\\begin{equation*}\n",
    "L_t^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\text{min}\\left(r_t(\\theta)\\hat{A}_t\\text{, clip}\\left( r_t(\\theta), 1-\\epsilon, 1+\\epsilon \\right)\\hat{A}_t\\right) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "L_t^{VF}(\\theta) = \\left( V_\\theta(s_t) - V_t^{targ} \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "$c_1$ and $c_2$ are hyperparameters, $S[\\pi](s_t)$ is an entropy bonus and the ratio $r_t(\\theta)$ is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} = \\text{exp}\\left(\\text{log }\\pi_\\theta(a_t | s_t) - \\text{ log }\\pi_{\\theta_{old}}(a_t|s_t)\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the PPO step for the given number of epochs and data\n",
    "def ppo_step(epochs, batch_size, states, actions, log_probs, advantages, pred_values, epsilon = 0.2, c_1 = 0.5, c_2 = 0.001):\n",
    "\t# In every epoch\n",
    "\tfor _ in range(epochs):\n",
    "\t\t# Iterate over generated batches:\n",
    "\t\tfor _states, _actions, _log_probs, _advantages, _values in generate_batches(states, actions, log_probs, advantages, pred_values, batch_size):\n",
    "\t\t\t# Calculate the new log_probabilities, evaluation and entropy\n",
    "\t\t\tnew_log_prob, state_eval, dist_entropy = model.evaluate(_states, _actions)\n",
    "\n",
    "\t\t\t# Evaluate the value function loss\n",
    "\t\t\tl_vf = mse_loss(state_eval, _values).mean()\n",
    "\n",
    "\t\t\t# Evaluate the clipped actor loss\n",
    "\t\t\tratio = (new_log_prob-_log_probs).exp()\n",
    "\t\t\tl_clip = torch.min(ratio*_advantages, torch.clamp(ratio, 1-epsilon, 1+epsilon)*_advantages).mean()\n",
    "\t\t\t\n",
    "\t\t\t# Evaluate the objective function (note that we need to swap signs, as the algorithm minimizes the objective)\n",
    "\t\t\tloss = (-l_clip + c_1*l_vf - c_2*dist_entropy).mean()\n",
    "\n",
    "\t\t\tprint(loss)\n",
    "\n",
    "\t\t\t# Perform the optimization step\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor(0.2747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0328, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2511, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0415, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s = torch.from_numpy(np.random.random((10, 9))).float()\n",
    "a = torch.from_numpy((np.random.random((10, 1))*9).astype(int)).float()\n",
    "log = torch.from_numpy(np.random.random((10))).float()\n",
    "adv = torch.from_numpy(np.random.random(10)).float()\n",
    "pvv = torch.from_numpy(np.random.random(10)).float()\n",
    "print(a.size(0))\n",
    "ppo_step(10, 3, s, a, log, adv, pvv)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a283dd9a253759e7df9c9142bbdf863a8a5c2da97ed0cffee6dbf6a32555f09f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
