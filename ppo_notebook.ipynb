{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "import numpy as np\n",
    "from numpy import random, sqrt\n",
    "\n",
    "# Network libraries\n",
    "import torch\n",
    "import torch.nn.functional as F, torch.nn as nn, torch.optim as optim\n",
    "from torch.distributions import Categorical as Categorical\n",
    "\n",
    "# Game library\n",
    "from TicTacToe import initialState, randomState, printState, drawState, move, possibleMoves, possibleMovesMask, isOver, gameScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Advantage Estimate (GAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAE is calculated using the formula:\n",
    "\n",
    "$ \\begin{equation*}\n",
    "\\hat{A_t}^{GAE(\\gamma, \\lambda)} = (1-\\lambda)\\left( \\hat{A}^{(1)}_t + \\lambda\\hat{A}^{(2)}_t + \\lambda^2\\hat{A}^{(3)}_t + ...\\right) = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta^V_{t+l}\n",
    "\\end{equation*}$\n",
    "\n",
    "where $\\delta^V_{t}$ is the TD residual defined by:\n",
    "\n",
    "$ \\begin{equation*}\n",
    "\\delta^V_{t} = r_t + \\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "Note that by the definition, we can easily evaulate $ \\hat{A_t}^{GAE(\\gamma, \\lambda)} $ based on $ \\hat{A_{t+1}}^{GAE(\\gamma, \\lambda)} $:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{A_t}^{GAE(\\gamma, \\lambda)} = \\delta^V_t + \\gamma\\lambda \\hat{A}_{t+1}^{GAE(\\gamma, \\lambda)} \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae(values, rewards, lmbda = 0.95, gamma = 0.99):\n",
    "\t# Initialize the advangate\n",
    "\tadv = 0\n",
    "\t# Initialize the results\n",
    "\tresults = []\n",
    "\n",
    "\t# Starting with the last action, iterate over actions and calculate the advantages\n",
    "\tfor t in reversed(range(len(rewards))):\n",
    "\t\t# Calculate the current value of delta\n",
    "\t\tdelta = rewards[t] + gamma * values[t+1] - values[t]\n",
    "\t\t# Update the current advantage\n",
    "\t\tadv = delta + gamma * lmbda * adv\n",
    "\t\t# Add the advantage to the results\n",
    "\t\tresults.append(adv)\n",
    "\t\n",
    "\t# Reverse the results to obtain the right order\n",
    "\tresults.reverse()\n",
    "\n",
    "\t# Return the results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "\t# Initialize the actor and critic\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(ActorCritic, self).__init__()\n",
    "\n",
    "\t\t# Actor\n",
    "\t\tself.actor = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 8, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(8, 32, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(32, 9)\n",
    "\t\t)\n",
    "\n",
    "\t\t# Critic\n",
    "\t\tself.critic = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 8, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(8, 32, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(32, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\t\n",
    "\t# Instead of using a forward function, we will split it into \n",
    "\t# one action for the actor, and one for the critic\n",
    "\tdef forward(self):\n",
    "\t\traise NotImplementedError\n",
    "\t\n",
    "\n",
    "\t# For a given state, return an action according to a current policy and the log_prob\n",
    "\t# of performing this action\n",
    "\tdef act(self, state):\n",
    "\t\t# Change the state into suitable format\n",
    "\t\ttensor_state = torch.from_numpy(state.reshape(-1, 1, 3,3)).float()\n",
    "\n",
    "\t\t# Calculate the initial output of the actor\n",
    "\t\tprobs = self.actor(tensor_state)\n",
    "\t\t\n",
    "\t\t# Apply the invalid action masking\n",
    "\t\tprobs = torch.where(possibleMovesMask(state, 1), probs, torch.tensor([-1e8]*9))\n",
    "\n",
    "\t\t# Apply the softmax function to obtain probabilities\n",
    "\t\tprobs = self.softmax(probs)\n",
    "\n",
    "\t\t# Create a distribution\n",
    "\t\tdist = Categorical(probs)\n",
    "\n",
    "\t\t# Pick an action using the generated distribution\n",
    "\t\taction = dist.sample()\n",
    "\n",
    "\t\t# Evaluate the log_prob of the chosen action\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\n",
    "\t\t# Return the action and the log_probabilties\n",
    "\t\treturn action.detach(), log_prob.detach()\n",
    "\n",
    "\n",
    "\t# For a given state and action, return:\n",
    "\t# \t- \tthe log_probabilities of the action, according to the current policy, \n",
    "\t# \t-\tthe estimated value of the state, according to the critic,\n",
    "\t#\t-\tthe entropy of the distribution\n",
    "\tdef evaluate(self, state, action):\n",
    "\t\t# Change the state into suitable format\n",
    "\t\ttensor_state = torch.from_numpy(state.reshape(-1, 1, 3,3)).float()\n",
    "\n",
    "\t\t# Calculate the initial output of the actor\n",
    "\t\tprobs = self.actor(tensor_state)\n",
    "\t\t\n",
    "\t\t# Apply the invalid action masking\n",
    "\t\tprobs = torch.where(possibleMovesMask(state, 1), probs, torch.tensor([-1e8]*9))\n",
    "\n",
    "\t\t# Apply the softmax function to obtain probabilities\n",
    "\t\tprobs = self.softmax(probs)\n",
    "\n",
    "\t\t# Create a distribution\n",
    "\t\tdist = Categorical(probs)\n",
    "\n",
    "\t\t# Evaluate the log_prob of the chosen action\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\n",
    "\t\t# Calculate the entropy of the distribution\n",
    "\t\tdist_entropy = dist.entropy()\n",
    "\n",
    "\t\t# Calculate the evaluation of the position\n",
    "\t\tstate_eval = self.critic(tensor_state)\n",
    "\n",
    "\t\t# Return the action and the log_probabilties\n",
    "\t\treturn log_prob, state_eval, dist_entropy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a283dd9a253759e7df9c9142bbdf863a8a5c2da97ed0cffee6dbf6a32555f09f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
